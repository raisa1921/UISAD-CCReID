{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93772be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributed as dist\n",
    "from tools.eval_metrics import evaluate, evaluate_with_clothes\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "ltcc_cc_ids = [6,8,9,11,57,58,137,105,106,78,29,79,82,83,31,90,46,59,60,16,17,143,110,118,24,25,52,28,33,63,101,134,142,145,65,20,32,37,74,125,84,104,141,120,72]\n",
    "ltcc_sc_ids = [1,12,80,139,107,108,112,114,49,55,14,62,67,26,99,126,102,19,23,121,77,119,73,68,38,147,148,113,41,42]\n",
    "cam2label = {'A': 0, 'B': 1, 'C': 2}\n",
    "\n",
    "VID_DATASET = ['ccvid']\n",
    "\n",
    "global_config = None\n",
    "\n",
    "def concat_all_gather(tensors, num_total_examples):\n",
    "    '''\n",
    "    Performs all_gather operation on the provided tensor list.\n",
    "    '''\n",
    "    outputs = []\n",
    "    for tensor in tensors:\n",
    "        tensor = tensor.cpu() \n",
    "        tensors_gather = [tensor.clone() for _ in range(dist.get_world_size())]\n",
    "        dist.all_gather(tensors_gather, tensor)\n",
    "        output = torch.cat(tensors_gather, dim=0).cpu()\n",
    "        # truncate the dummy elements added by DistributedInferenceSampler\n",
    "        outputs.append(output[:num_total_examples])\n",
    "    return outputs\n",
    "\n",
    "def to_rgb(img, mean=np.array([0.485, 0.456, 0.406]), std=np.array([0.229, 0.224, 0.225])):\n",
    "    mean = np.tile(mean, (img.size(1), img.size(2), 1))\n",
    "    std = np.tile(std, (img.size(1), img.size(2), 1))\n",
    "    # print (mean.shape, std.shape, img.shape)\n",
    "    img = torch.transpose(torch.transpose(img, 0, 1), 1, 2).numpy()\n",
    "    \n",
    "    img = (((img * std) + mean) * 255.0)\n",
    "    img = img[:,:,::-1]\n",
    "    return img\n",
    "\n",
    "def show_res(img_s, clos, unclos, conts, aa_s, bb_s, cc_s):\n",
    "    for idx,(a,b,c,d, aa, bb, cc) in enumerate(zip(img_s, clos, unclos, conts, aa_s, bb_s, cc_s)):\n",
    "        a = to_rgb(a)\n",
    "        b = to_rgb(b, np.array([0.5, 0.5, 0.5]), np.array([0.5, 0.5, 0.5]))\n",
    "        c = to_rgb(c, np.array([0.5, 0.5, 0.5]), np.array([0.5, 0.5, 0.5]))\n",
    "        d = to_rgb(d, np.array([0]), np.array([1]))\n",
    "        aa = to_rgb(aa, np.array([0.5, 0.5, 0.5]), np.array([0.5, 0.5, 0.5]))\n",
    "        bb = to_rgb(bb, np.array([0.5, 0.5, 0.5]), np.array([0.5, 0.5, 0.5]))\n",
    "        cc = to_rgb(cc, np.array([0]), np.array([1]))\n",
    "        global global_config\n",
    "        if not os.path.exists(global_config+'/show/'):\n",
    "            os.makedirs(global_config+'/show/')\n",
    "        cv2.imwrite(global_config+'/show/'+str(idx)+'_origin'+'.jpg', a)\n",
    "        cv2.imwrite(global_config+'/show/'+str(idx)+'_clo'+'.jpg', b)\n",
    "        cv2.imwrite(global_config+'/show/'+str(idx)+'_unclo'+'.jpg', c)\n",
    "        cv2.imwrite(global_config+'/show/'+str(idx)+'_cont'+'.jpg', d)\n",
    "\n",
    "        cv2.imwrite(global_config+'/show/'+str(idx)+'_clo_tar'+'.jpg', aa)\n",
    "        cv2.imwrite(global_config+'/show/'+str(idx)+'_unclo_tar'+'.jpg', bb)\n",
    "        cv2.imwrite(global_config+'/show/'+str(idx)+'_cont_tar'+'.jpg', cc)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_img_feature(model, dataloader):  \n",
    "    features, pids, camids, clothes_ids = [], torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "    for batch_idx, (imgs, batch_pids, batch_camids, batch_clothes_ids, batch_img_path) in enumerate(tqdm_notebook(dataloader)):\n",
    "        flip_imgs = torch.flip(imgs, [3])\n",
    "        imgs, flip_imgs = imgs.to(device), flip_imgs.to(device)\n",
    "        _, batch_features = model(imgs)\n",
    "        _, batch_features_flip = model(flip_imgs)\n",
    "        batch_features += batch_features_flip \n",
    "        batch_features = F.normalize(batch_features, p=2, dim=1) \n",
    "\n",
    "        features.append(batch_features.cpu())\n",
    "        pids = torch.cat((pids, batch_pids.cpu()), dim=0)       \n",
    "        camids = torch.cat((camids, batch_camids.cpu()), dim=0)\n",
    "        clothes_ids = torch.cat((clothes_ids, batch_clothes_ids.cpu()), dim=0)\n",
    "\n",
    "    features = torch.cat(features, 0)\n",
    "\n",
    "    return features, pids, camids, clothes_ids\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vid_feature(model, dataloader, vid2clip_index, data_length):\n",
    "    # In build_dataloader, each original test video is split into a series of equilong clips.\n",
    "    # During test, we first extact features for all clips\n",
    "    clip_features, clip_pids, clip_camids, clip_clothes_ids = [], torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "    for batch_idx, (vids, batch_pids, batch_camids, batch_clothes_ids) in enumerate(dataloader):\n",
    "        if (batch_idx + 1) % 200==0:\n",
    "            logger.info(\"{}/{}\".format(batch_idx+1, len(dataloader)))\n",
    "        vids = vids.to(device)\n",
    "        batch_features = model(vids)\n",
    "        clip_features.append(batch_features.cpu())\n",
    "        clip_pids = torch.cat((clip_pids, batch_pids.cpu()), dim=0)\n",
    "        clip_camids = torch.cat((clip_camids, batch_camids.cpu()), dim=0)\n",
    "        clip_clothes_ids = torch.cat((clip_clothes_ids, batch_clothes_ids.cpu()), dim=0)\n",
    "    clip_features = torch.cat(clip_features, 0)\n",
    "\n",
    "    # Use the averaged feature of all clips split from a video as the representation of this original full-length video\n",
    "    features = torch.zeros(len(vid2clip_index), clip_features.size(1)).to(device)\n",
    "    clip_features = clip_features.to(device)\n",
    "    pids = torch.zeros(len(vid2clip_index))\n",
    "    camids = torch.zeros(len(vid2clip_index))\n",
    "    clothes_ids = torch.zeros(len(vid2clip_index))\n",
    "    for i, idx in enumerate(vid2clip_index):\n",
    "        features[i] = clip_features[idx[0] : idx[1], :].mean(0)\n",
    "        features[i] = F.normalize(features[i], p=2, dim=0)\n",
    "        pids[i] = clip_pids[idx[0]]\n",
    "        camids[i] = clip_camids[idx[0]]\n",
    "        clothes_ids[i] = clip_clothes_ids[idx[0]]\n",
    "    features = features.cpu()\n",
    "\n",
    "    return features, pids, camids, clothes_ids\n",
    "\n",
    "\n",
    "def infer(config, model, queryloader, galleryloader, dataset, topk=10):\n",
    "    logger = logging.getLogger('reid.infer')\n",
    "    global global_config\n",
    "    global_config = config.OUTPUT\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "    local_rank = dist.get_rank()\n",
    "    # Extract features \n",
    "    if config.DATA.DATASET in VID_DATASET:\n",
    "        qf, q_pids, q_camids, q_clothes_ids = extract_vid_feature(model, queryloader, \n",
    "                                                                  dataset.query_vid2clip_index,\n",
    "                                                                  len(dataset.recombined_query))\n",
    "        gf, g_pids, g_camids, g_clothes_ids = extract_vid_feature(model, galleryloader, \n",
    "                                                                  dataset.gallery_vid2clip_index,\n",
    "                                                                  len(dataset.recombined_gallery))\n",
    "    else:\n",
    "        qf, q_pids, q_camids, q_clothes_ids = extract_img_feature(model, queryloader)\n",
    "        gf, g_pids, g_camids, g_clothes_ids = extract_img_feature(model, galleryloader)\n",
    "        torch.cuda.empty_cache()\n",
    "    qf = qf[:, 0:config.MODEL.FEATURE_DIM-config.MODEL.CLOTHES_DIM]\n",
    "    gf = gf[:, 0:config.MODEL.FEATURE_DIM-config.MODEL.CLOTHES_DIM]\n",
    "    torch.cuda.empty_cache() \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    logger.info(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))    \n",
    "    logger.info(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "    logger.info('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    # Compute distance matrix between query and gallery\n",
    "    since = time.time()\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "    qf, gf = qf.to(device), gf.to(device) \n",
    "    # Cosine similarity\n",
    "    for i in range(m):\n",
    "        distmat[i] = (- torch.mm(qf[i:i+1], gf.t())).cpu()\n",
    "    distmat = distmat.numpy()\n",
    "    q_pids, q_camids, q_clothes_ids = q_pids.numpy(), q_camids.numpy(), q_clothes_ids.numpy()\n",
    "    g_pids, g_camids, g_clothes_ids = g_pids.numpy(), g_camids.numpy(), g_clothes_ids.numpy()\n",
    "    time_elapsed = time.time() - since\n",
    "    logger.info('Distance computing in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    q_paths = [data for data, _, _, _ in dataset.query]\n",
    "    g_paths = [data for data, _, _, _ in dataset.gallery]\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    for q_idx, query_path in enumerate(q_paths):\n",
    "        out_file = query_path.replace(config.DATA.ROOT[0:-1], config.OUTPUT).replace('.png', '.txt').replace('.jpg', '.txt')\n",
    "        if not os.path.exists(os.path.dirname(out_file)):\n",
    "            os.makedirs(os.path.dirname(out_file))\n",
    "        if q_idx == 0:\n",
    "            print (\"Save to:\", os.path.dirname(out_file))\n",
    "        assert out_file.endswith('.txt')\n",
    "\n",
    "        with open(out_file, 'w') as fp:\n",
    "            for g_idx in indices[q_idx, :]:\n",
    "                fp.write(g_paths[g_idx]+'\\n')\n",
    "        #Visulization\n",
    "    visualize_ranked_results_inference(distmat, q_paths, g_paths, config, topk=topk)\n",
    "    return\n",
    "\n",
    "def visualize_ranked_results_inference(distmat, q_paths, g_paths, config, width=128, height=256, topk=10, GRID_SPACING=10, QUERY_EXTRA_SPACING=90, BW=5):\n",
    "    \"\"\"Visualizes ranked results.\n",
    "\n",
    "    Supports both image-reid and video-reid.\n",
    "\n",
    "    For image-reid, ranks will be plotted in a single figure. For video-reid, ranks will be\n",
    "    saved in folders each containing a tracklet.\n",
    "\n",
    "    Args:\n",
    "        distmat (numpy.ndarray): distance matrix of shape (num_query, num_gallery).\n",
    "        dataset (tuple): a 2-tuple containing (query, gallery), each of which contains\n",
    "            tuples of (img_path(s), pid, camid).\n",
    "        data_type (str): \"image\" or \"video\".\n",
    "        width (int, optional): resized image width. Default is 128.\n",
    "        height (int, optional): resized image height. Default is 256.\n",
    "        save_dir (str): directory to save output images.\n",
    "        topk (int, optional): denoting top-k images in the rank list to be visualized.\n",
    "            Default is 10.\n",
    "    \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "\n",
    "    num_cols = topk + 1\n",
    "    grid_img = 255 * np.ones((height, num_cols * width + (topk - 1) * GRID_SPACING + QUERY_EXTRA_SPACING, 3),\n",
    "                             dtype=np.uint8) \n",
    "\n",
    "    for q_idx, qimg_path in tqdm.tqdm(enumerate(q_paths)): \n",
    "        qimg = cv2.imread(qimg_path)                                   \n",
    "        qimg = Image.fromarray(cv2.cvtColor(qimg, cv2.COLOR_BGR2RGB))  \n",
    "        qimg = cv2.cvtColor(np.asarray(qimg), cv2.COLOR_RGB2BGR)       \n",
    "\n",
    "        qimg = cv2.resize(qimg, (width, height))\n",
    "        grid_img[:height, :width, :] = qimg\n",
    "\n",
    "        if config.DATA.DATASET == 'ltcc':\n",
    "            pattern1 = re.compile(r'(\\d+)_(\\d+)_c(\\d+)')\n",
    "            pattern2 = re.compile(r'(\\w+)_c')\n",
    "            q_pid, _, q_camid = map(int, pattern1.search(qimg_path).groups())\n",
    "            q_cloid = pattern2.search(qimg_path).group(1)\n",
    "        elif config.DATA.DATASET == 'prcc':\n",
    "            q_pid = os.path.basename(os.path.dirname(qimg_path))\n",
    "            q_camid = os.path.basename(os.path.dirname(os.path.dirname(qimg_path)))\n",
    "            q_camid = cam2label[q_camid]\n",
    "            q_cloid = q_camid\n",
    "\n",
    "        if config.INFER.SHOW_CC == True:\n",
    "            if config.DATA.DATASET == 'ltcc' and q_pid not in ltcc_cc_ids:\n",
    "                continue\n",
    "        \n",
    "        if config.INFER.SHOW_CC == False:\n",
    "            if config.DATA.DATASET == 'ltcc' and q_pid not in ltcc_sc_ids:\n",
    "                continue\n",
    "        \n",
    "        cnt = 0\n",
    "        rank_idx = 1\n",
    "        for g_idx in indices[q_idx, :]:\n",
    "            gimg_path = g_paths[g_idx]\n",
    "\n",
    "            if config.DATA.DATASET == 'ltcc':\n",
    "                \n",
    "                g_pid, _, g_camid = map(int, pattern1.search(gimg_path).groups())\n",
    "                g_cloid = pattern2.search(gimg_path).group(1)\n",
    "            elif config.DATA.DATASET == 'prcc':\n",
    "                g_pid = os.path.basename(os.path.dirname(gimg_path))\n",
    "                g_camid = os.path.basename(os.path.dirname(os.path.dirname(gimg_path)))\n",
    "                g_camid = cam2label[g_camid]\n",
    "                g_cloid = g_camid\n",
    "            \n",
    "            ## ALL show\n",
    "            if q_pid == g_pid and q_camid == g_camid: \n",
    "                continue           \n",
    "\n",
    "            ## CC show\n",
    "            if config.INFER.SHOW_CC == True:\n",
    "                if q_pid == g_pid and q_cloid == g_cloid: \n",
    "                    continue               \n",
    "            \n",
    "\n",
    "            if q_pid == g_pid:\n",
    "                border_color = (0, 255, 0) \n",
    "            else:\n",
    "                border_color = (0, 0, 255) \n",
    "                \n",
    "            gimg = cv2.imread(gimg_path)\n",
    "            gimg = Image.fromarray(cv2.cvtColor(gimg, cv2.COLOR_BGR2RGB))\n",
    "            gimg = cv2.cvtColor(np.asarray(gimg), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            gimg = cv2.resize(gimg, (width, height))\n",
    "            gimg = cv2.copyMakeBorder(gimg, BW, BW, BW, BW, cv2.BORDER_CONSTANT, value=border_color)\n",
    "            gimg = cv2.resize(gimg, (width, height))\n",
    "\n",
    "            start = rank_idx * width + (rank_idx - 1) * GRID_SPACING + QUERY_EXTRA_SPACING\n",
    "            end = (rank_idx + 1) * width + (rank_idx - 1) * GRID_SPACING + QUERY_EXTRA_SPACING\n",
    "            grid_img[:height, start:end, :] = gimg\n",
    "\n",
    "            rank_idx += 1\n",
    "            if rank_idx > topk:\n",
    "                break\n",
    "        if cnt == 0:\n",
    "            cnt += 1\n",
    "        cv2.imwrite(qimg_path.replace(config.DATA.ROOT[0:-1], config.OUTPUT), grid_img)\n",
    "\n",
    "\n",
    "def infer_prcc(config, model, queryloader_same, queryloader_diff, galleryloader, dataset, topk=10):\n",
    "    logger = logging.getLogger('reid.infer')\n",
    "    global global_config\n",
    "    global_config = config.OUTPUT\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "    # Extract features for query set\n",
    "    qsf, qs_pids, qs_camids, qs_clothes_ids = extract_img_feature(model, queryloader_same)\n",
    "    qdf, qd_pids, qd_camids, qd_clothes_ids = extract_img_feature(model, queryloader_diff)\n",
    "    # Extract features for gallery set\n",
    "    gf, g_pids, g_camids, g_clothes_ids = extract_img_feature(model, galleryloader)\n",
    "    torch.cuda.empty_cache() \n",
    "    qsf = qsf[:, 0:config.MODEL.FEATURE_DIM-config.MODEL.CLOTHES_DIM] \n",
    "    qdf = qdf[:, 0:config.MODEL.FEATURE_DIM-config.MODEL.CLOTHES_DIM] \n",
    "    gf = gf[:, 0:config.MODEL.FEATURE_DIM-config.MODEL.CLOTHES_DIM]   \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    logger.info(\"Extracted features for query set (with same clothes), obtained {} matrix\".format(qsf.shape)) \n",
    "    logger.info(\"Extracted features for query set (with different clothes), obtained {} matrix\".format(qdf.shape)) \n",
    "    logger.info(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "    logger.info('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    # Compute distance matrix between query and gallery\n",
    "    m, n, k = qsf.size(0), qdf.size(0), gf.size(0)\n",
    "    distmat_same = torch.zeros((m, k))\n",
    "    distmat_diff = torch.zeros((n, k))\n",
    "    qsf, qdf, gf = qsf.to(device), qdf.to(device), gf.to(device)\n",
    "    # Cosine similarity\n",
    "    for i in range(m):\n",
    "        distmat_same[i] = (- torch.mm(qsf[i:i+1], gf.t())).cpu()\n",
    "    for i in range(n):\n",
    "        distmat_diff[i] = (- torch.mm(qdf[i:i+1], gf.t())).cpu()\n",
    "    distmat_same = distmat_same.numpy()\n",
    "    distmat_diff = distmat_diff.numpy()\n",
    "    qs_pids, qs_camids, qs_clothes_ids = qs_pids.numpy(), qs_camids.numpy(), qs_clothes_ids.numpy()\n",
    "    qd_pids, qd_camids, qd_clothes_ids = qd_pids.numpy(), qd_camids.numpy(), qd_clothes_ids.numpy()\n",
    "    g_pids, g_camids, g_clothes_ids = g_pids.numpy(), g_camids.numpy(), g_clothes_ids.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    if config.INFER.SHOW_CC:\n",
    "        distmat = distmat_diff\n",
    "        q_paths = [data for data, _, _, _ in dataset.query_diff] \n",
    "        indices = np.argsort(distmat, axis=1)                    \n",
    "    else:                                                        \n",
    "        distmat = distmat_same                                   \n",
    "        q_paths = [data for data, _, _, _ in dataset.query_same]\n",
    "        indices = np.argsort(distmat, axis=1)\n",
    "    g_paths = [data for data, _, _, _ in dataset.gallery]\n",
    "\n",
    "    for q_idx, query_path in enumerate(q_paths):\n",
    "        out_file = query_path.replace(config.DATA.ROOT[0:-1], config.OUTPUT).replace('.png', '.txt').replace('.jpg', '.txt')\n",
    "        if not os.path.exists(os.path.dirname(out_file)):\n",
    "            os.makedirs(os.path.dirname(out_file))\n",
    "        if q_idx == 0:\n",
    "            print (\"Save to:\", os.path.dirname(out_file))\n",
    "        assert out_file.endswith('.txt')\n",
    "\n",
    "        with open(out_file, 'w') as fp:\n",
    "            for g_idx in indices[q_idx, :]:\n",
    "                fp.write(g_paths[g_idx]+'\\n')\n",
    "        #Visulization\n",
    "    visualize_ranked_results_inference(distmat, q_paths, g_paths, config, topk=topk)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
