{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a463e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f37a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributed as dist\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from configs.default_img_visualization import get_img_config\n",
    "from models.img_resnet import ResNet50\n",
    "from data import build_dataloader\n",
    "from models.classifier import Classifier, NormalizedClassifier\n",
    "from PIL import Image\n",
    "import gc\n",
    "from tools.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006c8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    'demo_single_image.ipynb',\n",
    "    '--cfg', 'configs/res50_cels_cal.yaml',\n",
    "    '--dataset', 'prcc', \n",
    "    '--img_path', 'C:/Users/USER/OneDrive/Documents/My Nural Net/Person Re-Identification/dataset/prcc/rgb/test/B/030/cropped_rgb442.jpg',\n",
    "    '--weights', 'C:/Users/USER/OneDrive/Documents/My Nural Net/Person Re-Identification/outputs/prcc/eval/best_model.pth.tar', \n",
    "    '--gpu', '0',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f178a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faaa6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Train clothes-changing re-id model with clothes-based adversarial loss')\n",
    "    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file')\n",
    "    # Datasets\n",
    "    parser.add_argument('--root', type=str, help=\"your root path to data directory\")\n",
    "    parser.add_argument('--dataset', type=str, default='ltcc', help=\"ltcc, prcc, vcclothes, last, deepchange\")\n",
    "    # Miscs\n",
    "    parser.add_argument('--img_path', type=str, help='path to the image')\n",
    "    parser.add_argument('--weights', type=str, help='path to the weights')\n",
    "    parser.add_argument('--gpu', type=str, default='0', help='gpu id')\n",
    "\n",
    "    args, unparsed = parser.parse_known_args()\n",
    "    config = get_img_config(args)\n",
    "    return config, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_img_feature(model, img):\n",
    "    flip_img = torch.flip(img, [3])\n",
    "    img, flip_img = img.to(device), flip_img.to(device)\n",
    "    _, batch_features = model(img)\n",
    "    _, batch_features_flip = model(flip_img)\n",
    "    batch_features += batch_features_flip\n",
    "    batch_features = F.normalize(batch_features, p=2, dim=1)\n",
    "    features = batch_features.cpu()\n",
    "\n",
    "    return features\n",
    "\n",
    "config, args = parse_option()\n",
    "\n",
    "dict = torch.load(args.weights)\n",
    "model = ResNet50(config)\n",
    "\n",
    "model.load_state_dict(dict['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((config.DATA.HEIGHT, config.DATA.WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image = Image.open(args.img_path)\n",
    "image_tensor = data_transforms(image)\n",
    "input_batch = image_tensor.unsqueeze(0)  \n",
    "\n",
    "feature = extract_img_feature(model, input_batch)\n",
    "\n",
    "print(\"Input Image:\", args.img_path, \" Output Feautre:\", feature)\n",
    "print(feature.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27083cc-e641-4af3-8446-822adb83621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.feature_maps = None\n",
    "\n",
    "        # Register hooks\n",
    "        self.target_layer.register_forward_hook(self.save_feature_maps)\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "\n",
    "    def save_feature_maps(self, module, input, output):\n",
    "        self.feature_maps = output\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate_cam(self, input_tensor):\n",
    "        # Forward pass\n",
    "        base_f, f = self.model(input_tensor)\n",
    "        # Compute the loss with respect to a dummy target\n",
    "        loss = f.norm()  \n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Get gradients and feature maps\n",
    "        gradients = self.gradients.cpu().data.numpy()[0]\n",
    "        feature_maps = self.feature_maps.cpu().data.numpy()[0]\n",
    "\n",
    "        # Compute the weights\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        cam = np.sum(weights[:, np.newaxis, np.newaxis] * feature_maps, axis=0)\n",
    "\n",
    "        # Apply ReLU to the CAM\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        cam = cv2.resize(cam, (input_tensor.size(2), input_tensor.size(3)))\n",
    "        return cam\n",
    "\n",
    "# Set up Grad-CAM\n",
    "target_layer = model.layer4  \n",
    "grad_cam = GradCAM(model, target_layer)\n",
    "\n",
    "input_batch = input_batch.to(device)\n",
    "\n",
    "# Generate the heatmap\n",
    "heatmap = grad_cam.generate_cam(input_batch)\n",
    "heatmap = cv2.resize(heatmap, (image.size[0], image.size[1]))\n",
    "\n",
    "# Convert the heatmap to an image\n",
    "#heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_RAINBOW) \n",
    "heatmap = np.float32(heatmap) / 255\n",
    "overlayed_img = heatmap + np.array(image.resize((heatmap.shape[1], heatmap.shape[0]))) / 255\n",
    "overlayed_img = overlayed_img / np.max(overlayed_img)\n",
    "\n",
    "# Plot the original image and the heatmap\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Original Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(overlayed_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Heatmap Overlay\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
